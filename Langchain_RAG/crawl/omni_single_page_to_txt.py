from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from urllib.parse import urlparse
import os
import time

# Danh s√°ch URL c·∫ßn crawl
URLS_TO_CRAWL = [
    "https://omni.co/platform",
    "https://omni.co/business-intelligence",
    "https://omni.co/embedded-analytics",
    "https://omni.co/integrations",
    "https://omni.co/customer-case-studies",
    "https://omni.co/customer-support",
    "https://omni.co/about",
    "https://omni.co/security",
    "https://omni.co/demos",
]

OUTPUT_FOLDER = "omni"  # Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(OUTPUT_FOLDER, exist_ok=True)

# C·∫•u h√¨nh Selenium (Headless mode ƒë·ªÉ ch·∫°y nhanh h∆°n)
chrome_options = Options()
chrome_options.add_argument("--headless")  # Ch·∫°y kh√¥ng giao di·ªán
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--window-size=1920x1080")

# Kh·ªüi t·∫°o WebDriver
driver = webdriver.Chrome(options=chrome_options)

def sanitize_filename(url):
    """Chuy·ªÉn ƒë·ªïi URL th√†nh t√™n file h·ª£p l·ªá."""
    parsed_url = urlparse(url)
    filename = parsed_url.path.replace("/", "_").strip("_")
    return filename if filename else "index"

def save_content_to_file(url, content):
    """L∆∞u n·ªôi dung t·ª´ URL v√†o file trong th∆∞ m·ª•c omni."""
    try:
        filename = sanitize_filename(url) + ".txt"
        filepath = os.path.join(OUTPUT_FOLDER, filename)
        with open(filepath, "w", encoding="utf-8") as file:
            file.write(content)
        print(f"‚úÖ Saved: {filepath}")
    except Exception as e:
        print(f"‚ùå Error saving {url}: {e}")

def crawl_page_content(url):
    """Truy c·∫≠p URL, l·∫•y n·ªôi dung trong th·∫ª <main> v√† l∆∞u v√†o file."""
    try:
        driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "main")))

        main_content = driver.find_element(By.TAG_NAME, "main").text.strip()
        if main_content:
            save_content_to_file(url, main_content)
        else:
            print(f"‚ö†Ô∏è No content found in <main> for {url}")

    except Exception as e:
        print(f"‚ùå Error processing {url}: {e}")

# Ch·∫°y crawler tr√™n danh s√°ch URL
for url in URLS_TO_CRAWL:
    print(f"üîç Crawling: {url}")
    crawl_page_content(url)
    time.sleep(1)  # Ngh·ªâ 1 gi√¢y ƒë·ªÉ tr√°nh b·ªã ch·∫∑n

# ƒê√≥ng tr√¨nh duy·ªát
driver.quit()
print("‚úÖ Done!")
